diff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py
index 96d356b8b38..c43183cae78 100644
--- a/benchmarks/dynamo/common.py
+++ b/benchmarks/dynamo/common.py
@@ -480,7 +480,7 @@ def maybe_mark_step(args):
         xm.mark_step()
 
 
-def speedup_experiment(args, model_iter_fn, model, example_inputs, **kwargs):
+def speedup_experiment(args, model_iter_fn, model, optimized_model, example_inputs, **kwargs):
     """
     Measure speedups over eager.
 
@@ -521,7 +521,7 @@ def speedup_experiment(args, model_iter_fn, model, example_inputs, **kwargs):
             )
         else:
             frozen_model_iter_fn = torch._dynamo.run(model_iter_fn)
-
+        
         for rep in trange(args.repeat, desc="running benchmark"):
             inputs = (
                 randomize_input(copy.deepcopy(example_inputs))
@@ -549,7 +549,7 @@ def speedup_experiment(args, model_iter_fn, model, example_inputs, **kwargs):
 
             with maybe_mark_profile(p=p, mark="actual"):
                 timings[rep, 1], actual_output = timed(
-                    model,
+                    optimized_model,
                     frozen_model_iter_fn,
                     inputs,
                     return_result=True,
@@ -559,7 +559,7 @@ def speedup_experiment(args, model_iter_fn, model, example_inputs, **kwargs):
 
     if args.export_profiler_trace:
         name = args.profiler_trace_name + "_" + model.name + ".json"
-        name = os.path.join(torch._dynamo.config.base_dir, name)
+        name = os.path.join('/workspace/pytorch', name)
         p.export_chrome_trace(name)
     median = np.median(timings, axis=0)
     speedup = median[0] / median[1]
@@ -2164,7 +2164,17 @@ class BenchmarkRunner:
             reset_rng_state()
             model_copy = None
             try:
-                model_copy = self.deepcopy_and_maybe_ddp(model)
+                if self.args.backend == "ipex":
+                    print('In ipex!!!\n')
+                    optimized_model = copy.deepcopy(model)
+                    import intel_extension_for_pytorch as ipex
+                    if self.args.bfloat16 or self.args.amp:
+                        optimized_model = ipex.optimize(optimized_model, dtype=torch.bfloat16, weights_prepack=False)
+                    elif self.args.float32:
+                        optimized_model = ipex.optimize(optimized_model, weights_prepack=False)
+                else:
+                    optimized_model = model
+                model_copy = self.deepcopy_and_maybe_ddp(optimized_model)
                 self.init_optimizer(name, current_device, model_copy.parameters())
                 correct_result = self.run_n_iterations(
                     model_copy, clone_inputs(example_inputs)
@@ -2184,7 +2194,7 @@ class BenchmarkRunner:
             reset_rng_state()
             model_copy = None
             try:
-                model_copy = self.deepcopy_and_maybe_ddp(model)
+                model_copy = self.deepcopy_and_maybe_ddp(optimized_model)
                 self.init_optimizer(name, current_device, model_copy.parameters())
                 correct_rerun_result = self.run_n_iterations(
                     model_copy, clone_inputs(example_inputs)
@@ -2379,6 +2389,18 @@ class BenchmarkRunner:
     def run_performance_test(
         self, name, model, example_inputs, optimize_ctx, experiment, tag=None
     ):
+        if self.args.backend == "ipex":
+            optimized_model = copy.deepcopy(model)
+            fusion_path = os.environ["FUSION_PATH"]
+            import intel_extension_for_pytorch as ipex
+            if self.args.bfloat16:
+                optimized_model = ipex.optimize(optimized_model, dtype=torch.bfloat16, weights_prepack=False)
+            elif self.args.float32:
+                optimized_model = ipex.optimize(optimized_model, weights_prepack=False)
+            ipex._set_compiler_backend(fusion_path) # jit path: "torchscript", inductor path: "inductor".:
+
+        else:
+            optimized_model = model
         if self.args.xla:
             with self.pick_grad(name, self.args.training):
                 return experiment(*self.maybe_cast(model, example_inputs))
@@ -2410,6 +2432,7 @@ class BenchmarkRunner:
 
         # Cast the model to float16/float32 as necessary
         model, example_inputs = self.maybe_cast(model, example_inputs)
+        optimized_model, example_inputs = self.maybe_cast(model, example_inputs)
 
         # Use distributed wrapping as necessary
         model = self.deepcopy_and_maybe_ddp(model)
@@ -2435,7 +2458,7 @@ class BenchmarkRunner:
                 aot_compilation_time = 0
 
             dynamo_latency, dynamo_peak_mem, dynamo_stats = warmup(
-                optimized_model_iter_fn, model, example_inputs, "dynamo"
+                optimized_model_iter_fn, optimized_model, example_inputs, "dynamo"
             )
 
             compilation_time = dynamo_latency - eager_latency + aot_compilation_time
@@ -2480,7 +2503,7 @@ class BenchmarkRunner:
 
             if not hasattr(model, name):
                 model.name = name
-            results.append(experiment(model, example_inputs, **experiment_kwargs))
+            results.append(experiment(model, optimized_model, example_inputs, **experiment_kwargs))
             return " ".join(map(str, results))
 
     def minify_model(
@@ -2502,7 +2525,7 @@ class BenchmarkRunner:
         if self.args.output_directory:
             repro_dir = self.args.output_directory
         else:
-            repro_dir = torch._dynamo.config.base_dir
+            repro_dir = '/workspace/pytorch'
 
         try:
             shutil.move("repro.py", f"{repro_dir}/{name}_repro.py")
@@ -3456,7 +3479,7 @@ def run(runner, args, original_dir=None):
             output_filename = os.path.join(args.output_directory, output_filename)
         else:
             output_filename = os.path.join(
-                torch._dynamo.config.base_dir, output_filename
+                '/workspace/pytorch', output_filename
             )
 
     if args.find_batch_sizes and args.only:
diff --git a/benchmarks/dynamo/huggingface.py b/benchmarks/dynamo/huggingface.py
index ddbe43967f3..698eff78e42 100755
--- a/benchmarks/dynamo/huggingface.py
+++ b/benchmarks/dynamo/huggingface.py
@@ -6,6 +6,7 @@ import re
 import subprocess
 import sys
 import warnings
+import intel_extension_for_pytorch as ipex
 
 import torch
 from common import BenchmarkRunner, download_retry_decorator, main, reset_rng_state
diff --git a/benchmarks/dynamo/runner.py b/benchmarks/dynamo/runner.py
index 504e354a58b..44818b487a7 100755
--- a/benchmarks/dynamo/runner.py
+++ b/benchmarks/dynamo/runner.py
@@ -86,6 +86,7 @@ TABLE = {
         "trt": "--inference -n100 --speedup-trt ",
         "ts_nvfuser_cudagraphs": "--inference --backend=cudagraphs_ts ",
         "inductor": "--inference -n50 --inductor ",
+"ipex": "--backend=ipex --inference ",
         "inductor_no_cudagraphs": "--inference -n50 --inductor --disable-cudagraphs ",
         "inductor_max_autotune": "--inference -n50 --inductor --inductor-compile-mode max-autotune ",
         "inductor_max_autotune_no_cudagraphs": (
diff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py
index 7a29267b05c..8bdda8375e7 100755
--- a/benchmarks/dynamo/timm_models.py
+++ b/benchmarks/dynamo/timm_models.py
@@ -6,6 +6,7 @@ import re
 import subprocess
 import sys
 import warnings
+import intel_extension_for_pytorch as ipex
 
 import torch
 from common import BenchmarkRunner, download_retry_decorator, main
diff --git a/benchmarks/dynamo/torchbench.py b/benchmarks/dynamo/torchbench.py
index 633b9f60805..3f7c80d9be0 100755
--- a/benchmarks/dynamo/torchbench.py
+++ b/benchmarks/dynamo/torchbench.py
@@ -6,6 +6,7 @@ import os
 import re
 import sys
 import warnings
+import intel_extension_for_pytorch as ipex
 from os.path import abspath, exists
 
 import torch
